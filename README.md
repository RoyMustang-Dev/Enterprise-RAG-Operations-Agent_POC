# Enterprise RAG Operations Agent

## ğŸš€ Project Overview
This project is a **production-grade Retrieval-Augmented Generation (RAG) system** designed to operate as an autonomous knowledge agent for enterprise environments. Unlike simple chatbots, this system focuses on **explicit orchestration**, **verifiable data ingestion**, and **operational transparency**.

It allows users to ingest complex enterprise documents (PDF, DOCX, TXT) and crawl dynamic websites, building a unified knowledge base that can be queried with high precision.

### Layer-Wise Architecture Data Flow
*(The following diagram visually maps the precise physical inputs and structural outputs transmitted across each logical layer of the RAG system.)*

![Layer-Wise Execution Architecture](./assets/architecture_layered.png)

## âœ¨ Comprehensive Key Features
- **Deterministic Multi-Agent Orchestration**: Powered by **LangGraph** to maintain robust `AgentState` dictionaries, strictly preventing runtime hallucinations and bounding LLM logic through an explicit DAG topology.
- **Adaptive Execution Planner**: Advanced node interceptor actively deciding optimal reasoning models, bounding deep metrics explicitly mapping `openai/gpt-oss-120b` natively only when heuristic boundaries are explicitly exceeded by `ComplexityClassifier` logic arrays.
- **RLAIF Hallucination Correction Loop**: Sovereign Verifier bounds. Draft generations flagged (`is_hallucinated=True`) systematically route through a forced error-correction execution DAG natively suppressing loose citations dynamically.
- **Micro-Model Supervisor Routing**: Triages user intent instantly (<0.0s) using high-speed, lightweight models (`llama-3.1-8b-instant`) to either bypass the database for smalltalk or engage the heavy RAG analytics.
- **Cross-Encoder Reranking**: Utilizes `bge-reranker-large` locally to semantically filter top-30 low-confidence similarity chunks down to the absolute top 5 most relevant documents, executing a mathematically pristine `Math Sigmoid` filter.
- **Independent Fact Verification**: Secondary LLM logic (`Sarvam M`) acts as a sovereign auditor, redacting claims generated by the Core Brain (`llama-3.3-70b-versatile`) if they cannot be verified against the native source text.
- **Dynamic Multi-Source Ingestion**: Asynchronously processes standard enterprise documents (PDF, DOCX) and executes deep web-crawls using headless **Playwright** browsers to extract dynamic text from JavaScript-heavy Single Page Applications (SPAs).
- **Decoupled Application Persistence**: Completely decoupled conversational tracking utilizing `SQLite` histories scaling automatically globally isolated from frontend UI states.
- **Enterprise Traceability**: Fully embedded telemetry engine exporting detailed JSONL audit logs capturing token densities (`tokens_input/output`), execution latencies natively mapped to components, RLAIF Reward bounds, Hardware probes, and hallucination verdicts.
- **Hardware Agnostic Runtime**: Actively probes local memory structures (RAM/VRAM) to strictly tune background worker ingestion queue arrays instantly matching GPU embeddings batching organically natively preventing physical architecture bottlenecks globally.

## ğŸ—ï¸ Project Architecture

```text
enterprise-rag-agent/
â”‚
â”œâ”€â”€ app/             # Enterprise Vertical Slice Architecture
â”‚   â”œâ”€â”€ api/             # HTTP layer ONLY (FastAPI endpoints, swagger definitions)
â”‚   â”‚   â””â”€â”€ routes.py      # Core endpoints (/chat, /ingest/files, /ingest/crawler, /feedback) with rate-limiting & background telemetry injection.
â”‚   â”œâ”€â”€ core/            # Cross-cutting primitives (Types, exceptions, telemetry mapping)
â”‚   â”‚   â”œâ”€â”€ rate_limit.py  # TokenBucket in-memory rate limiter protecting external HTTP endpoints.
â”‚   â”‚   â”œâ”€â”€ telemetry.py   # ObservabilityLayer emitting TelemetryLogRecord metrics natively to JSONL.
â”‚   â”‚   â””â”€â”€ types.py       # Global Pydantic typed dictionaries (AgentState, TelemetryLogRecord).
â”‚   â”œâ”€â”€ supervisor/      # ReAct brain (Router map, Intent Detector, Adaptive Planner)
â”‚   â”‚   â”œâ”€â”€ intent.py      # Semantic Intent Classifier (llama-3.1-8b) routing queries (RAG vs Code vs Greeting).
â”‚   â”‚   â”œâ”€â”€ planner.py     # Adaptive Planner intercepting paths, controlling dynamic routing, and setting reasoning effort limits.
â”‚   â”‚   â””â”€â”€ router.py      # ExecutionGraph DAG Controller piping data through the Prompt Guard -> Planning -> Context Merge -> End Execution.
â”‚   â”œâ”€â”€ prompt_engine/   # Guardrails & Prompts (Llama-Guard, Rewriter, Templates)
â”‚   â”‚   â”œâ”€â”€ guard.py       # PromptInjectionGuard (llama-prompt-guard-2) short-circuiting malicious injections.
â”‚   â”‚   â””â”€â”€ rewriter.py    # PromptRewriter (gpt-oss-120b) mathematically interpolating ambiguous inputs using chat history.
â”‚   â”œâ”€â”€ ingestion/       # Full Data Pipeline (Crawler, Loader, Chunker, Sync->Async loops)
â”‚   â”‚   â”œâ”€â”€ chunker.py     # RecursiveCharacterTextSplitter strictly matching hardware encoding limits.
â”‚   â”‚   â”œâ”€â”€ crawler_service.py # Scalable headless Playwright worker intelligently scraping SPA Javascript pages natively.
â”‚   â”‚   â”œâ”€â”€ loader.py      # PyMuPDF extraction engine parsing physical PDF/DOCX multi-column layouts robustly.
â”‚   â”‚   â””â”€â”€ pipeline.py    # IngestionPipeline orchestrator syncing parsed chunks into Qdrant batched dynamically by the HardwareProbe. 
â”‚   â”œâ”€â”€ retrieval/       # Search Mechanics (Qdrant DB, BAAI Embeddings, Cross-Encoder Reranker, Metadata Extractor)
â”‚   â”‚   â”œâ”€â”€ embeddings.py  # Hardware-aware EmbeddingModel (BAAI/bge-large-en) with LRU caching natively mapping Tensors dynamically to GPU/CPU.
â”‚   â”‚   â”œâ”€â”€ hybrid_search.py # Optional BM25 lexical reranker applied conditionally over dense embeddings.
â”‚   â”‚   â”œâ”€â”€ metadata_extractor.py # Strictly bound JSON extractor explicitly validating specific schema tags for rigorous Qdrant $eq filtering.
â”‚   â”‚   â”œâ”€â”€ reranker.py    # Cross-Encoder (bge-reranker-large) heavily isolating the exact Top-5 semantic chunks utilizing robust Sigmoid Math Activation.
â”‚   â”‚   â””â”€â”€ vector_store.py # Persistent QdrantStore adapter ensuring isolated multi-tenant Cloud or Local interactions globally.
â”‚   â”œâ”€â”€ agents/          # Execution Workers (RAG Agent DAG, Code Agent, Smalltalk Bypass)
â”‚   â”‚   â”œâ”€â”€ coder.py       # Code-Specialized analytical MoE mapped to qwen-32b.
â”‚   â”‚   â””â”€â”€ rag.py         # Orchestrator chaining Extraction -> Rerank -> Adaptive Synthesis -> Fact Verification -> RLHF loop natively.
â”‚   â”œâ”€â”€ reasoning/       # Core Logic Brain (Llama-70B Synthesis, Sarvam Verifier, Citation Formatter)
â”‚   â”‚   â”œâ”€â”€ complexity.py  # Low-latency classifier measuring dense semantics and triggering Dynamic Model Overrides dynamically based on query heuristics.
â”‚   â”‚   â”œâ”€â”€ formatter.py   # ResponseFormatter mapping Streamlit UI visual citations.
â”‚   â”‚   â”œâ”€â”€ synthesis.py   # SynthesisEngine executing context-bound responses. Includes sovereign Halucination Correction Loop integration.
â”‚   â”‚   â””â”€â”€ verifier.py    # HallucinationVerifier (Sarvam M) actively redacting claims fundamentally failing extraction mappings natively.
â”‚   â”œâ”€â”€ rlhf/            # Data Flywheel (Feedback Store, Reward Model logs)
â”‚   â”‚   â”œâ”€â”€ feedback.py        # RLHF structural API payload validations.
â”‚   â”‚   â”œâ”€â”€ feedback_store.py  # Feedback Database routines binding user Thumbs-Up metrics natively to session telemetry blocks.
â”‚   â”‚   â””â”€â”€ reward_model.py    # Automatic RLAIF generation executing a math array: Grounding(0.4) Actionability(0.3) Conciseness(0.2) Coherence(0.1).
â”‚   â””â”€â”€ infra/           # Systems Infrastructure (Rate Limits, Hardware GPU Probing, DB Init)
â”‚       â”œâ”€â”€ database.py    # Native SQLite database strictly bounding App Ingestion Trackers, RLHF bounds, and explicitly decoupled Multi-Turn Chat session histories.
â”‚       â”œâ”€â”€ hardware.py    # HardwareProbe actively measuring core execution metrics enforcing worker multipliers mapping locally or externally natively.
â”‚       â”œâ”€â”€ otel.py        # OpenTelemetry implementation natively wrapping trace loops over Fast API architectures.
â”‚       â””â”€â”€ redis.py       # Global Distributed rate-limiting stub for scaled multi-worker persistence blocks.
â”‚
â”œâ”€â”€ frontend/                 # User Interface
â”‚   â””â”€â”€ app.py                # Streamlit Dashboard interacting directly via HTTP requests to `/api/v1/chat`.
â”‚
â”œâ”€â”€ data/                     # Root Data Persistence Storage
â”‚   â”œâ”€â”€ crawled_docs/         # Scraped Playwright raw outputs.
â”‚   â”œâ”€â”€ uploaded_docs/        # User manual file uploads.
â”‚   â”œâ”€â”€ qdrant_storage/       # Internal decoupled Qdrant persistence.
â”‚   â””â”€â”€ audit/                # Persistent JSONL files dynamically catching global system latency, parameters, and telemetry strings dynamically.
â”‚
â””â”€â”€ tests/                    # System Verification
```

## ğŸ› ï¸ Technology Stack

| Component | Tech | Reason for Choice |
| :--- | :--- | :--- |
| **Language** | Python 3.11 | Industry standard for AI/ML engineering. |
| **Orchestration** | LangGraph | Phase 3/4: State-based multi-agent orchestration for robust NLP routing. |
| **Frontend** | Streamlit | Rapid prototyping and interactive data visualization. |
| **Backend API** | FastAPI | Phase 5: High-performance, async-native REST API (Headless SaaS Architecture). |
| **Core Brain LLM**| Groq (`llama-3.3-70b-versatile` & `gpt-oss-120b`) | Phase 8: Dynamic Reasoning Controller routing high/low complexity queries automatically to save inference tokens. |
| **Intent/Speed LLM**| Groq (`llama-3.1-8b-instant`) | Phase 3/5: Near-instantaneous intent routing and JSON-mode metadata extraction logic. |
| **Independent Verifier**| Sarvam AI (`sarvam-m`) | Phase 9: Secondary LLM layer to mathematically verify fact citations. |
| **Code Execution**| Groq (`qwen/qwen3-32b`) | Phase 4: Purpose-built analytical MoE strictly handling codebase logic routing. |
| **Embeddings** | BAAI/bge-large-en-v1.5 + Reranker | State-of-the-art open-source semantic generation. |
| **Vector Store** | Qdrant | Scalable cloud-first vector index (Replaced flat FAISS files). |
| **PDF Processing** | PyMuPDF (fitz) | Fastest and most accurate text extraction for PDFs. |
| **Web Crawling** | **Playwright** + BeautifulSoup | Handles client-side JS rendering for modern SPAs. |

## ğŸš€ Installation & Usage

### Prerequisites
- Python 3.10+
- OS: Windows/Linux/Mac

### Setup
1.  **Clone the repository:**
    ```bash
    git clone <repo-url>
    cd Enterprise-RAG-Operations-Agent_POC
    ```

2.  **Create and activate virtual environment:**
    ```bash
    python -m venv venv
    venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    playwright install chromium
    ```

4.  **Environment Setup (`.env`):**
    ```env
    GROQ_API_KEY=your_groq_api_key_here
    SARVAM_API_KEY=your_sarvam_api_key_here
    HF_TOKEN=your_huggingface_read_token_here
    QDRANT_API_KEY=your_qdrant_cloud_key_here
    QDRANT_URL=your_qdrant_cloud_url_here
    ```

### Running the Application

To run the full stack, you need to open two separate terminals.

5.  **Start the Backend API (Uvicorn):**
    Open your first terminal, ensure your virtual environment is activated, and run the FastAPI server:
    ```bash
    # Activate environment (if not already active)
    venv\Scripts\activate      # Windows
    # source venv/bin/activate # Mac/Linux
    
    # Start the backend server
    uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ```
    âœ… **Swagger API Docs:** Access the interactive endpoints at `http://localhost:8000/docs`

6.  **Start the Frontend Portal (Streamlit):**
    Open a *second* terminal window, activate the virtual environment again, and launch the UI:
    ```bash
    # Activate environment in the new terminal
    venv\Scripts\activate      # Windows
    # source venv/bin/activate # Mac/Linux
    
    # Start the Streamlit dashboard
    streamlit run frontend/app.py
    ```
    âœ… **Agent Dashboard UI:** Access the portal at `http://localhost:8501`

---

## ğŸ“ˆ Integration Phases

### âœ… Phase 1: Ingestion Engine
**Goal:** Build a robust, fault-tolerant ingestion pipeline strictly isolated within the `app/ingestion/` vertical slice.
- Migrated physical document loaders (`PyMuPDF`) and web scrapers into a unified ingestion stream.
- Handled complex SPA crawling natively via a highly concurrent **Playwright** implementation.
- Integrated `RecursiveCharacterTextSplitter` to handle LLM token-aware sliding window data segmentation.

#### ğŸ—ï¸ Architecture (Phase 1)
![Phase 1 Architecture](./assets/architecture_phase1.png)

---

### âœ… Phase 2: Embeddings & Qdrant Vector Store
**Goal:** Transition entirely to a managed Cloud Vector Database (Qdrant) and implement Cross-Encoder Reranking inside `app/retrieval/`.
- Executed strict L2 Cosine Distance mapping via `BAAI/bge-large-en-v1.5` embeddings on the GPU.
- Implemented `bge-reranker-large` inside `app/retrieval/reranker.py` to evaluate the Top 30 vectors and return strictly the Top 5.
- Completely abandoned legacy FAISS persistence in favor of native Qdrant payloads for enterprise multi-tenancy.

#### ğŸ—ï¸ Architecture (Phase 2)
![Phase 2 Architecture](./assets/architecture_phase2.png)

---

### âœ… Phase 3 & 4: Agentic Architecture Overhaul
**Goal:** Remove fragile procedural RAG scripts and replace them with a LangGraph multi-agent orchestrator resilient to logical exceptions.

#### ğŸ—ï¸ Architecture (Phase 3 & 4)
![Phase 3 & 4 Architecture](./assets/architecture_phase3_4.png)

*(The top section of the diagram illustrates the **Phase 4 Agent Implementations**, including the high-speed Groq Supervisor routing intent dynamically. The bottom section illustrates the **Phase 3 RAG Agent Refactoring**, decoupling the vector grounded extraction and synthesis into isolated LangGraph worker nodes natively in `app/agents/`.)*

#### ğŸ› ï¸ Step-by-Step Implementation
1.  **State Management (LangGraph):**
    -   Implemented a strictly typed `AgentState` dictionary globally mapping `app/core/types.py`.
    -   Secures LLM conversational history organically across multi-turn sessions, bounding the schema execution.
    -   Exposes `latency_optimizations` metrics tracing the autonomous model routing.
2.  **Supervisor Semantic Routing:**
    -   Configured the `app/supervisor/intent.py` module using `llama-3.1-8b-instant` for explicit instant triage.
    -   Seamlessly switches execution paths between bypassing retrieval for smalltalk or initiating exhaustive agentic queries.
3.  **Specialized Domain Execution Agents:**
    -   **Smalltalk Bypass:** Natively routes via `app/supervisor/router.py` executing instant responses without querying Qdrant.
    -   **RAG DAG Execution:** Initiates `app/agents/rag.py` to chain retrieval, dynamic synthesis (routing `70B` or `120B` depending on complexity), and verification (`Sarvam`).
4.  **UI Alignment & Optimization:**
    -   Streamlit UI explicitly mapped to visually render the deep execution graphs dynamically as they execute in the backend.
    -   Silenced huggingface PyTorch tensor initialization outputs to keep system logs enterprise-clean.

---

### âœ… Phase 5: API-First Decoupling & SaaS Dependency Validation
**Goal:** Transition entirely into a fully headless, vertical-slice architecture where HTTP interfaces strictly decouple from core logic.
- Implemented `/api/v1/chat` and `/api/v1/ingest/crawler` completely isolated in `app/api/routes.py`.
- Constructed explicit `pydantic` request payloads, using `typing.Literal` constraints to map dynamic Swager UI dropdowns enforcing correct model execution options dynamically natively.
- Safely stripped the legacy monolithic `backend/` directory footprint out of the executable domain.

### âœ… Phase 6: Enterprise Telemetry & Independent Assertion Verification
**Goal:** Establish sovereign accountability loops auditing every node within the LangGraph DAG, alongside verifying facts line-by-line via secondary AI layers.
- Deployed a completely independent Fact Verifier node powered by `Sarvam M` residing at `app/reasoning/verifier.py` to cross-examine LLM claims organically against the source Qdrant snippets.
- Implemented JSONL audit telemetry logging all execution vertices mapping prompt costs, latencies, and hallucination logic trees explicitly for future RLHF.

---

### âœ… Phase 7: Enterprise Vector Database Upgrade (Qdrant)
**Goal:** Eliminate the scalability bottleneck of local FAISS flat-files by migrating to a managed Cloud Vector Database capable of handling high-dimensional semantic search at scale for multi-tenant isolation.
- Selected **Qdrant** for its massive "1GB Free" Cloud Tier (~1M vectors) and open-source Docker adaptability, ensuring zero-cost scaling and no vendor lock-in.
- Upgrading `app/ingestion/chunker.py` to utilize a fully token-aware `RecursiveCharacterTextSplitter`. This prevents critical data truncation that plagued the legacy 512-word methodology.
- Configuring explicit BGE `L2 Normalization` inside `app/retrieval/vector_store.py` to guarantee mathematically flawless Cosine Similarity calculations across the remote Qdrant index.

---

## ğŸ—ï¸ The Complete 11-Step RAG Agentic Architecture

Below represents the exhaustive integration of the enterprise RAG standards we implemented natively into the LangGraph execution flow, strictly isolating vector similarity from intelligent reasoning.

1. **Prompt Injection & Safety Guard:** Protects the execution graph from system prompt extraction and RAG poisoning using `llama-prompt-guard-2-86m`.
2. **Prompt Rewriter / Query Expansion:** Mathematically expands ambiguous user queries natively utilizing `openai/gpt-oss-120b` (wrapped with defensive regex parsing for exact JSON stability).
3. **Intent Detection Supervisor:** A high-speed classifier (`llama-3.1-8b-instant`) strictly routing the execution state without blocking the async API loop.
4. **Agent Dispatch / Smalltalk Bypass:** Routes trivial greetings to a lightweight responder, bypassing the expensive vector database.
5. **Dynamic Metadata Extraction:** Leverages `llama-3.1-8b-instant` to stably parse the user's natural language into strict JSON `$eq` filters, mapping directly to Qdrant payloads.
6. **Vector Similarity Search (Top 30):** Executes a high-recall L2 Cosine Distance search utilizing `BAAI/bge-large-en-v1.5` embeddings on the GPU.
7. **Cross-Encoder Reranking (Top 5):** Evaluates the top 30 chunks through a rigorous semantic cross-encoder algorithm. We strictly apply a Math Sigmoid Activation function `1 / (1 + math.exp(-logit))` to transform raw pipeline logits into bounds checking >0.35 probability before truncating down to Top 5.
8. **Core Reasoning Synthesis (Dynamic Routing):** A discrete Complexity Analyzer logic gate dictates generation. If the query exhibits intense reasoning demands (Word counts >40 or multi-hop logic gates like "contrast/analyze"), execution overrides instantly to `openai/gpt-oss-120b`. Otherwise, it falls back to the low-latency `llama-3.3-70b-versatile`.
9. **Independent Fact Verifier:** A sovereign model (`Sarvam M`) audits the generated text line-by-line exclusively against the source chunks, redacting unsourced claims natively mapping visually onto UI strikethroughs.
10. **Formatter & Citation Modeler:** Injects physical markdown URL and Document links natively into the structured stream response for Streamlit UI rendering.
11. **Telemetry & RLHF Auditing:** Traps latency matrices, token bounds, dynamic routing models, and hallucination verdicts natively into the `audit_logs.jsonl` pipeline.

### Visual Architecture Diagram (The Execution DAG)

![11-Step RAG Execution Architecture](./assets/architecture_11_steps.png)

### Strict Step-By-Step Execution Flow

To ensure complete understanding of the pipeline's deterministic routing, here is the exact input/output life cycle of a conversational turn:

1. **User Payload [Input: `JSON` -> Layer: HTTP API]**
   - The client system inputs a JSON payload directly into the `/api/v1/chat` Swagger/API endpoint.
   ![Step 1: HTTP API Ingestion](./assets/step01_http_api_ingestion.png)
2. **Security Interception [Input: `str` -> Layer: Prompt Guard (`llama-prompt-guard-2-86m`)]**
   - **Action:** Mathematically screens for adversarial structure or data poisoning attempts.
   - **Output [Boolean + Category]:** If `is_malicious=True`, the DAG short-circuits and immediately returns an HTTP 403 Security Exception.
   ![Step 2: Prompt Guard Security](./assets/step02_prompt_guard_security.png)
3. **Query Expansion [Input: `str` -> Layer: Prompt Rewriter (`gpt-oss-120b`)]**
   - **Action:** Ingests previous conversational turns and synthesizes ambiguous queries (e.g., "how much does it cost?") into standalone logical sentences based on history.
   - **Output [JSON payload]:** Optimized standard and deep reasoning variants of the prompt.
   ![Step 3: Query Expansion Rewriter](./assets/step03_query_expansion_rewriter.png)
4. **Semantic Triage [Input: `str` -> Layer: Intent Supervisor (`llama-3.1-8b-instant`)]**
   - **Action:** Triggers an ultra-fast classification against the query to define its rigid `AgentState` intent.
   - **Output [Enum String]:** `"rag_question"`, `"greeting"`, `"code_request"`, or `"out_of_scope"`.
   ![Step 4: Semantic Intent Triage](./assets/step04_semantic_intent_triage.png)
5. **Path Divergence [Input: `Enum String` -> Layer: DAG Controller]**
   - **Action:** If `"greeting"`, instantly returns a lightweight JSON response payload. If `"code_request"`, routes directly to the specialized `qwen-32b` analytical MoE.
   ![Step 5: DAG Path Divergence](./assets/step05_dag_path_divergence.png)
6. **Query Grounding [Input: `str` -> Layer: Metadata Extractor (`llama-3.1-8b-instant`)]**
   - **Action:** Analyzes the prompt for natural language bounds (e.g. "Only documents from 2024 about engineering") and parses them into strict JSON `$eq` objects.
   - **Output [JSON Schema]:** Strictly formatted `models.Filter` maps.
   ![Step 6: Metadata Filter Extraction](./assets/step06_metadata_filter_extraction.png)
7. **Similarity Search [Input: `str` + `JSON` -> Layer: Qdrant Database]**
   - **Action:** Generates an embedding tensor via `BAAI/bge-large-en` and queries Qdrant DB for the 30 semantically closest L2 distances, restricted by the applied metadata bounds.
   - **Output [List[Dict]]:** 30 raw chunks of enterprise text data.
   ![Step 7: Qdrant Similarity Search](./assets/step07_qdrant_similarity_search.png)
8. **Mathematical Pruning [Input: `List[Dict]` -> Layer: Cross-Encoder Reranker]**
   - **Action:** Forces a semantic logic matrix across all 30 chunks, outputting raw numerical logits. A mathematical Sigmoid expansion function converts logits to rigorous `0.0-1.0` probabilities, dumping anything `< 0.35` threshold.
   - **Output [List[Dict]]:** The absolute most vital 5 text chunks.
   ![Step 8: Cross-Encoder Reranker](./assets/step08_cross_encoder_reranker.png)
9. **Heuristic Engine Control [Input: `str` + `int` -> Layer: Complexity Analyzer]**
   - **Action:** Sweeps the query length, multiple logic hops ("compare/analyze"), and array density.
   - **Output [Target Model]:** Overrides routing. Triggers `gpt-oss-120b` if intensive, otherwise defaults to `llama-3.3-70b-versatile` to heavily stabilize latency.
   ![Step 9: Complexity Heuristic Analyzer](./assets/step09_complexity_heuristic_analyzer.png)
10. **Native Synthesis [Input: `str` + `List[Dict]` + `Model` -> Layer: Reasoning API]**
    - **Action:** Blocks the target LLM strictly forcing it to build an answer derived solely from the 5 validated chunks.
    - **Output [JSON String]:** The formatted RAG answer, Confidence score % (0.0-1.0), and chunk provenance integers.
    ![Step 10: Reasoning Synthesis](./assets/step10_reasoning_synthesis.png)
11. **Hallucination Redaction [Input: `JSON String` + `List[Dict]` -> Layer: Fact Verifier (`Sarvam M`)]**
    - **Action:** Computes a secondary sovereign inference sweep, isolating every generated sentence and mapping it back against the source 5 chunks.
    - **Output [Boolean + Array]:** Any logic leaps are captured, setting `is_hallucinated=True` and returning unsupported claims.
    ![Step 11: Sarvam Fact Verifier](./assets/step11_sarvam_fact_verifier.png)
12. **API JSON Construction [Input: `AgentState` -> Layer: Model Formatter]**
    - **Action:** Formats the final RAG answer, telemetry diagnostics, chat history, and provenance metadata into a strict Pydantic model dictionary array for network transmission.
    - **Output [FastAPI Response]:** A structured JSON HTTP 200 response returned natively to the API caller.
    ![Step 12: JSON API Formatter](./assets/step12_json_api_formatter.png)

---

## ğŸ›‘ Operational Challenges & Advanced Resolutions
*The following documents critical friction points tackled during stabilization of the Enterprise Stack.*

### 1. Stateless LangGraph Execution Paths
**Challenge:** Early iterations of the FastAPI payload successfully accepted conversational `chat_history` lists from the Streamlit UI, however, the Agentic Orchestrator (`app/supervisor/router.py`) routinely initialized an empty semantic array into the graph on execution. This completely wiped conversational retention from the LLM across multi-turn queries.
**Resolution:** The core `invoke` and `ainvoke` mappings were rewritten to natively unpack the user query and forcefully append both the user's string and the LLM's final generated logic sequentially back into the global `AgentState` dictionary before executing the final return constraint.

### 2. Catastrophic Metadata Dropping via Model Incompatibility
**Challenge:** The initial Extractor iteration deployed `qwen/qwen-2.5-32b` utilizing Groq's API and explicitly mandated a generic JSON format `response_format={"type": "json_object"}`. Groq API strictly isolates its managed JSON-mode architecture natively supporting only Llama3 and GPT-OSS models. This generated violent `JSONDecodeError` failures, causing the application to execute defensive exception blocks that effectively eradicated metadata routing from the Vector DB search arrays.
**Resolution:** By rigorously consulting the **Groq API Documentation**, we identified that strictly formatted `{"type": "json_object"}` payloads are architecturally unsupported natively by their proprietary Qwen MoE endpoints. Consequently, the architecture was dynamically overridden. The Extractor and Rewriters were upgraded to heavily JSON-stabilized structures (`llama-3.1-8b-instant` and `gpt-oss-120b`). Additionally, a deep defensive `import re` regular expression fallback was implemented instantly caching any payloads hallucinogenically wrapping native JSON inside Markdown ( \```json ... \``` ).

### 3. Native Reranker Logit Truncation Collisions
**Challenge:** The local BAAI Cross-Encoder executed flawlessly, predicting distances on 30 vector nodes. However, the legacy script arbitrarily bounded `score > 0.35` straight against the raw generated predictions (Logits), which mathematically represent wildly unbounded raw values (-5.2 -> 8.5). This resulted in highly correct context bounds being immediately wiped mathematically leading to a catastrophic `[]` zero-chunk pipeline.
**Resolution:** We integrated absolute `math.exp()` normalizations. The raw model bounds were directly encapsulated inside a **Sigmoid Activation Function** `probability = 1 / (1 + math.exp(-logit))` instantly correcting the unbounded logits into pristine bounded decimals `0.0 -> 1.0` allowing the `< 0.35` prune function to accurately delete hallucinations without destroying context logic.

### 4. Spiking Inference Costs on Massive Concurrency 
**Challenge:** Initially locking the synthesis engine to `llama-3.3-70b-versatile` guaranteed maximum accuracy. However, sending small contextual responses across 100 concurrent workers forced extensive latency generation locks inside Groq.
**Resolution:** Deployed a discrete, heuristic-driven **Complexity Analyzer** natively into the RAG routing loop at Step 9. We systematically evaluate the exact raw token input structure, semantic depth constraints, and multi-hop triggers prior to generating the RAG output. We aggressively route standard generation queries to the extremely low-latency `llama-3.3-70b-versatile`, while preserving and upgrading intensive analytical arrays to the highly compute-dense logic constraints of `gpt-oss-120b`, saving tokens dynamically.
